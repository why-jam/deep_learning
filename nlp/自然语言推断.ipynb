{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "# 预处理\n",
    "def preprocess_nmt(text):\n",
    "    '''在字符与标点符号之间添加空格'''\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "    #用空格代替不间断空格（non-breaking space）\\xa0 是不间断空白符 \n",
    "    text = text.replace('\\u202f',' ').replace('\\xa0',' ')\n",
    "    #在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char,text[i-1]) \n",
    "          else char for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    label_set = {'entailment':0, 'contradiction':1 ,'neutral':2}\n",
    "    file_name = os.path.join(data_dir,'snli_1.0_train.csv'\n",
    "                            if is_train else 'snli_1.0_test.csv')\n",
    "    \n",
    "    data = pd.read_csv(file_name)[['gold_label','sentence1','sentence2']]\n",
    "    data = data[data['gold_label']!='-']\n",
    "    premises = data['sentence1'].map(preprocess_nmt).values\n",
    "    hypotheses = data['sentence2'].astype(str).map(preprocess_nmt).values\n",
    "    labels = data['gold_label'].map(label_set).values\n",
    "    return premises, hypotheses, labels\n",
    "\n",
    "\n",
    "is_train = True\n",
    "data_dir = 'archive'\n",
    "train_data = read_snli(data_dir, is_train=True)\n",
    "for x0,x1,y in zip(train_data[0][:3],train_data[1][:3],train_data[2][:3]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment       183416\n",
       "contradiction    183187\n",
       "neutral          182764\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file_name)[['gold_label','sentence1','sentence2']]\n",
    "data = data[data['gold_label']!='-']\n",
    "\n",
    "data['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和测试集中每类样本的数量\n",
      "第0类样本数量： 183416\n",
      "第1类样本数量： 183187\n",
      "第2类样本数量： 182764\n",
      "---------------\n",
      "第0类样本数量： 3368\n",
      "第1类样本数量： 3237\n",
      "第2类样本数量： 3219\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, is_train = False)\n",
    "print('训练集和测试集中每类样本的数量')\n",
    "for data in [train_data, test_data]:\n",
    "    for i in range(3):\n",
    "        print(f'第{i}类样本数量：',[row for row in data[2]].count(i))\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 定义加载数据集的类\n",
    "### 1.1.1数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    '''文本词元化'''\n",
    "    if token=='word':\n",
    "        lines = [line.split(' ') for line in lines]\n",
    "    elif token == 'char':\n",
    "        lines = [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR：未知词元类型：'+ token)\n",
    "    return lines\n",
    "\n",
    "import collections\n",
    "class Vocab():\n",
    "    def __init__(self,tokens=None, min_freq=0 ,reversed_token=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reversed_token is None:\n",
    "            reversed_token = []\n",
    "        counter = corpus_freq(tokens)\n",
    "        # 定义私有变量，只有当前类内的方法或函数可以访问，统计所有词的词频\n",
    "        self._token_freq = sorted(counter.items(), key = lambda x:x[1], \n",
    "                                reverse = True)\n",
    "        self.idx_to_token = ['<unk>'] + reversed_token\n",
    "        self.token_to_idx = {token:idx for token,idx \n",
    "                             in enumerate(self.idx_to_token)}\n",
    "        for (token,freq) in self._token_freq:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            else:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens, (list,tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_token(self, indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "    \n",
    "    @property #装饰器，作用是把类中的方法变成属性来进行调用\n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def token_freq(self):\n",
    "        return self._token_freq\n",
    "        \n",
    "def corpus_freq(tokens):\n",
    "    '''计算所有token的词频'''\n",
    "    if isinstance(tokens[0], list) or len(tokens)==0:\n",
    "        token = [token for line in tokens for token in line]\n",
    "    return collections.Counter(token)       \n",
    "\n",
    "def truncate_pad(line, num_step, padding_token):\n",
    "    '''对一个样本进行裁剪和填充，保证长度为num_step'''\n",
    "    if len(line) < num_step:\n",
    "        line += [padding_token]*(num_step - len(line))\n",
    "    else:\n",
    "        line = line[:num_step]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps , vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        # 根据训练集建立词表, 前提和假设，两个部分\n",
    "        all_premise_tokens = tokenize(dataset[0]) \n",
    "        all_hypotheses_tokens = tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocab(all_premise_tokens + all_hypotheses_tokens,\n",
    "                              min_freq=5, reversed_token=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab #采用预训练的词向量模型，比如fasttext，glove\n",
    "            \n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypotheses_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read '+ str(len(self.premises)) + ' examples')\n",
    "        \n",
    "    def _pad(self, lines):\n",
    "        \"\"\"输入tokens序列，进行idx转换并裁剪\"\"\"\n",
    "        return torch.tensor([truncate_pad(self.vocab[line], self.num_steps,\n",
    "                                        self.vocab['<pad>'])\n",
    "                            for line in lines])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19173"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_snli(batch_size, num_steps = 50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    # 读取数据集\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    # 自定义SNLI数据集\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    # 创建数据迭代器\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, \n",
    "                                             batch_size, shuffle = True)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set,\n",
    "                                           batch_size, shuffle = False)\n",
    "    return train_iter, test_iter, train_set.vocab\n",
    "\n",
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自然语言推断：使用注意力\n",
    "## 2.1 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_inputs, num_hiddens, flatten):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class Attend(nn.Module):\n",
    "    '''\n",
    "    将一个文本序列中的词元与另一个序列中的每个词元对齐\n",
    "    '''\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "        \n",
    "    def forward(self, A, B):\n",
    "        # A/B的形状：（批量大小，序列A/B的词元数，embed_size）\n",
    "        # f_A/f_B的形状：（批量大小，序列A/B的词元数，num_hiddens）\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        # e的形状：（批量大小，序列A的词元数，序列B的词元数）\n",
    "        e = torch.bmm(f_A, f_B.permute(0,2,1))\n",
    "        # beta的形状：（批量大小，序列A的词元数，embed_size），\n",
    "        # 意味着序列B被软对齐到序列A的每个词元(beta的第1个维度)\n",
    "        beta = torch.bmm(F.softmax(e,dim=-1), B)\n",
    "        # alpha的形状：（批量大小，序列B的词元数，embed_size），\n",
    "        # 意味着序列A被软对齐到序列B的每个词元(alpha的第1个维度)\n",
    "        alpha = torch.bmm(F.softmax(e.permute(0,2,1), dim=-1), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compare(nn.Module):\n",
    "    '''\n",
    "    将一个序列中的词元与该词元软对齐的另一个序列进行比较\n",
    "    '''\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "        \n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
    "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    '''\n",
    "    将两个求和结果的连结提供给函数（一个多层感知机），以获得逻辑关系的分类结果\n",
    "    '''\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "\n",
    "    def forward(self, V_A, V_B):\n",
    "        # 对两组比较向量分别求和\n",
    "        V_A = V_A.sum(dim=1)\n",
    "        V_B = V_B.sum(dim=1)\n",
    "        # 将两个求和结果的连结送到多层感知机中\n",
    "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------整合代码\n",
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n",
    "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "        # 有3种可能的输出：蕴涵、矛盾和中性\n",
    "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 训练和评估模型\n",
    "## 1. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_steps = 256, 50\n",
    "train_iter, test_iter, vocab = load_data_snli(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens = 100, 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Glove嵌入\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token:idx\n",
    "                            for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'],[]\n",
    "        data_dir = d2l.download_extract(embedding_name)\n",
    "        # GloVe网站：https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText网站：https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir,'vec.txt'), 'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token ,elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # 跳过标题信息，例如fastText中的首行\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "            \n",
    "        idx_to_vec = [[0]*len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "    \n",
    "    # 返回对应token的vecs向量\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token,self.unknown_idx)\n",
    "                  for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "glove_embedding = TokenEmbedding('glove.6b.100d')    \n",
    "\n",
    "# 取出了token对应的vec\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "net.embedding.weight.data.copy_(embeds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练和评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 4\n",
    "trainer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\") #不综合， 保留每个样本的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net,X,y,loss, trainer,device):\n",
    "    if isinstance(X,list):\n",
    "        X = [x.to(device) for x in X]\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = (pred.argmax(1)==y).sum()\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, trainer, num_epochs,device):\n",
    "    timer, num_batches = d2l.Timer(),len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch',xlim=[1,num_epochs],ylim=[0,1],\n",
    "                           legend=['train loss','train acc', 'test acc'])\n",
    "    net = net.to(device)\n",
    "#     num_batches = len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l,acc = train_batch(net,features, labels, loss,trainer,device)\n",
    "            metric.add(l ,acc ,labels.shape[0] ,labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_acc = 0\n",
    "            test_num = 0\n",
    "            for X, y in test_iter:\n",
    "                if isinstance(X,list):\n",
    "                    X = [x.to(device) for x in X]\n",
    "                else:\n",
    "                    X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = net(X)\n",
    "                test_acc += (pred.argmax(1)==y).sum()\n",
    "                test_num += y.numel()\n",
    "            animator.add(epoch + 1, (None, None, test_acc/test_num))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc/test_num:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, train_iter, test_iter, loss, trainer, num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- 预测\n",
    "\n",
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    \"\"\"预测前提和假设之间的逻辑关系\"\"\"\n",
    "    net.eval()\n",
    "    premise = torch.tensor(vocab[premise],device = device)\n",
    "    hypothesis = torch.tensor(vocab[hypothesis], device = device)\n",
    "    label = torch.argmax(net([premise.reshape(1,-1), \n",
    "                              hypothesis.reshape(-1,1)]), dim=1)\n",
    "    return 'entailment' if label==0 else (\n",
    "    'contradiction' if label == 1 else 'neural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
