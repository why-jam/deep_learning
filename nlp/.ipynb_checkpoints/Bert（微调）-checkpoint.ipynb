{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segment(token_a, token_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + token_a + ['<sep>']\n",
    "    segments = [0]*(len(tokens_a)+2)\n",
    "    if token_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1]*(len(tokens)+1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"Bert编码器：输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和\"\n",
    "    def __init__(self,vocab_size,num_hiddens,norm_shape,ffn_num_input,\n",
    "                ffn_num_hiddens,num_heads, num_layers,dropout,\n",
    "                 max_len = 1000, key_size=768,query_size=768,value_size=768,**kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2,num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\",d2l.EncoderBlock(\n",
    "                key_size,query_size,value_size,num_hiddens,norm_shape,\n",
    "                ffn_num_input,ffn_num_hiddens,num_heads,dropout,True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,max_len,num_hiddens))\n",
    "        \n",
    "    def forward(self,tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1],:]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "\n",
    "#\n",
    "tokens = torch.randint(0,vocab_size,(2,8))\n",
    "segments = torch.tensor([[0,0,0,0,1,1,1,1],\n",
    "                        [0,0,0,1,1,1,1,1]])\n",
    "encoded_X = encoder(tokens,segments,None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练任务\n",
    "## 1.掩蔽语言模型（Masked Language Modeling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([1, 5, 2, 6, 1, 5])\n",
      "torch.Size([6, 768])\n",
      "torch.Size([2, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"Bert的掩码语言模型任务\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens,num_inputs=768,**kwargs):\n",
    "        super(MaskLM,self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs,num_hiddens),\n",
    "                                nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                nn.Linear(num_hiddens,vocab_size))\n",
    "        \n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        print(num_pred_positions)\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        print(pred_positions)\n",
    "        batch_idx = torch.arange(0,batch_size)\n",
    "        # 假设batch_size=2，num_pred_positions=3\n",
    "        # 那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        print(masked_X.shape)\n",
    "        masked_X = masked_X.reshape(batch_size, num_pred_positions,-1)\n",
    "        print(masked_X.shape)\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "    \n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算真实标签mlm_Y与预测词元mlm_Y_hat之间的交叉熵损失\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下一句预测（Next Sentence Prediction）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"Bert的下一句预测任务,返回每个BERT输入序列的二分类预测\"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred,self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs,2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X的形状：(batchsize,num_hiddens)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "print(encoded_X.shape)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算两个二元分类的交叉熵损失\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整合代码\n",
    "预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"Bert模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape,ffn_num_input,\n",
    "                ffn_num_hiddens, num_heads, num_layers,dropout,\n",
    "                max_len = 1000, key_size=768,query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768, nsp_in_features=768):\n",
    "        super(BERTModel,self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout,max_len=max_len,\n",
    "                                  key_size=key_size,query_size=query_size,value_size=value)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features,num_hiddens),\n",
    "                                   nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "        \n",
    "    def forward(self, tokens, segments, valid_lens = None,\n",
    "               pred_positions = None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoder_X[:,0,:]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调BERT——自然语言推断\n",
    "## 1. 加载预训练的BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练的BERT\n",
    "# “bert.base”与原始的BERT基础模型一样大，需要大量的计算资源才能进行微调，\n",
    "# “bert.small”是一个小版本\n",
    "\n",
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip',\n",
       " 'c72329e68a732bef0452e4b96a1c341c8910f81f')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.DATA_HUB['bert.small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                         num_heads, num_layers, dropout, max_len, device):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "                                                     'vocab.json')))\n",
    "    vocab.token_to_idx = {token:idx \n",
    "                          for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape = [256],\n",
    "                        ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                        num_heads=4, num_layers=2, dropout=0.2,\n",
    "                        max_len = max_len, key_size=256, query_size=256,\n",
    "                        value_size=256, hid_in_features=256,\n",
    "                        mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert, vocab = load_pretrained_model('bert.small', num_hiddens=256, \n",
    "                                    ffn_num_hiddens=512, num_heads=4,\n",
    "                                   num_layers=2,dropout=0.1,max_len=512,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 微调BERT的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "#         pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "#         out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "#         all_token_ids = [\n",
    "#             token_ids for token_ids, segments, valid_len in out]\n",
    "#         all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "#         valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        all_token_ids ,ll_segments,valid_lens =\\\n",
    "            self._mp_worker(all_premise_hypothesis_tokens)\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "#         p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        p_tokens = [pre_hyp[0] for pre_hyp in premise_hypothesis_tokens]\n",
    "        h_tokens = [pre_hyp[1] for pre_hyp in premise_hypothesis_tokens]\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-79a71351bb75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mis_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'archive'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSNLIBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSNLIBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-d881c458d0b2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, max_len, vocab)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         (self.all_token_ids, self.all_segments,\n\u001b[1;32m---> 12\u001b[1;33m          self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'read '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_token_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' examples'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-d881c458d0b2>\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(self, all_premise_hypothesis_tokens)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mall_token_ids\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mll_segments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_lens\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mp_worker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_premise_hypothesis_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         return (torch.tensor(all_token_ids, dtype=torch.long),\n\u001b[0m\u001b[0;32m     25\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_segments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 torch.tensor(valid_lens))\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "# 预处理\n",
    "def preprocess_nmt(text):\n",
    "    '''在字符与标点符号之间添加空格'''\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "    #用空格代替不间断空格（non-breaking space）\\xa0 是不间断空白符 \n",
    "    text = text.replace('\\u202f',' ').replace('\\xa0',' ').lower()\n",
    "    #在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char,text[i-1]) \n",
    "          else char for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    label_set = {'entailment':0, 'contradiction':1 ,'neutral':2}\n",
    "    file_name = os.path.join(data_dir,'snli_1.0_train.csv'\n",
    "                            if is_train else 'snli_1.0_test.csv')\n",
    "    \n",
    "    data = pd.read_csv(file_name)[['gold_label','sentence1','sentence2']]\n",
    "    data = data[data['gold_label']!='-']\n",
    "    premises = data['sentence1'].map(preprocess_nmt).values\n",
    "    hypotheses = data['sentence2'].astype(str).map(preprocess_nmt).values\n",
    "    labels = data['gold_label'].map(label_set).values\n",
    "    return premises, hypotheses, labels\n",
    "\n",
    "\n",
    "is_train = True\n",
    "data_dir = 'archive'\n",
    "batch_size, max_len = 256, 64\n",
    "train_data = read_snli(data_dir, is_train=True)\n",
    "test_data = read_snli(data_dir, is_train=False)\n",
    "\n",
    "train_set = SNLIBERTDataset(train_data, max_len, vocab)\n",
    "test_set = SNLIBERTDataset(test_data, max_len, vocab)\n",
    "# train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "# test_iter = torch.utils.data.DataLoader(test_set, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 微调BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(sefl, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_X = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_X)\n",
    "        return self.output(self.hidden(encoded_X[:,0,:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BERTClassifier(bert)\n",
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
